<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Mining Source Code Repositories with Solr</title>
    <meta name="description" content="Importing Git Repositories into a Solr Index">
    <meta name="author" content="Gary Sieling">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">
    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">
    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
      document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <style>
      .reveal pre {
      width:110%;
      }

      #button0, #button1, #button2 {
        margin-top:2px;
        height: 62px;
        padding: 4px 26px;
        background: #4479BA;
        color: #FFF;
        font-size: 30pt;
      }

      #query0, #query1, #query2 {
          height: 44px;
          font-size: 24pt;
      }

      .loadingimage, .diffimage, .solrimage {
              border: 0px !important; 
              background: rgba(0, 0, 0, 0) !important;
              margin: 0px !important;
      }
    </style>
  </head>
  <body>
    <div class="reveal">
      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Mining Source Code Repositories with Solr</h1>
          <h3>Exploring Full-Text Search</h3>
          <p>
            <a href="http://www.garysieling.com/jug">www.garysieling.com/jug</a>
          </p>
          <p>
            <small><a href="http://www.garysieling.com">Gary Sieling</a> / <a href="http://twitter.com/garysieling">@garysieling</a></small>
          </p>
          <aside class="notes">
            <li>In this talk I’m going to discuss full-text indexing in Solr, which is an open-source Java tool that wraps Lucene. I’ll show you how to build up an example project which indexes the contents of git source code repositories. </li>
            <li>Some of you may be familiar with products like Atlassian’s Fisheye, or code search engines like Krugle, or used the Github search. The nice thing about these is they let you do more than the out-of-the box 'blame' tool in your version control software, like finding out who ported a feature between branches, or searching both git and cvs repositories. </li>
            <li>Since there are mature products for this type of thing, this talk isn’t intended to be a replacement for them; rather it’s a well contained way to understand full-text search, and to explore the design challenges in building ETL processes.</li>
          </aside>
        </section>
        <section>
          <h1>Overview</h1>
          <ul>
          <li>Demo</li>
          <li>How-to</li>
          <li>Inspiration</li>
          </ul>
          <aside class="notes">
          </aside>
          </section>
        <section>
          <h2>Motivation</h2>
          <p>
            Which engineer worked on a particular client/project/technology? 
          </p>
          <aside class="notes">
            <li>Since Wingspan does some consulting projects, it’s often valuable for engineers to look at a change and know if it was done for a particular client - for instance, if an API feature does not appear to be used, it may actually be in use by a client. That said, the person who may have done the change may not be the most knowledgeable about it, and you'd prefer to know who the lead for a project was.</li>
            <li>Similarly, when someone calls the company, it’s also helpful to know who to route phone calls to - especially since the person who picked up the phone could be a marketing or administrative person receiving the call and not someone with knowledge of the project.</li>
            <li>This is faceted search.</li>
          </aside>
          </section>
        <section>
          <h2>Project Structure</h2>
          <p>
          <img src="uml.png" />
          </p>
          <aside class="notes">
            <li>This is faceted search.</li>
          </aside>
          </section>

        <section>
        <h2>Search for Experts In...</h1>
<div style="width:960px; margin: 0px auto;"/>
<input type="text" id="query0" style="width:70%;padding:8px;margin:0px 5px;" value="solr"/><input type="button" id="button0" value="Go" /><br/>
</p>
<div id="results0">
</div>
<br/>
<br/>
<br/>
<br/>
<br/>
</div>
<aside class="notes">
Mark Robert Miller	(909)	- Apache foundation
Yonik Seeley	(718)	- Creator of Solr
Robert Muir	(530)	- Lucene Committer
Chris M Hostetter	(365)	- Solr/Lucene/Lucidworks
Steven Rowe	(273)	- Apache Lucene/Solr PMC member & committer
          </aside>
        </section>
          
        <section>
        <h2>Search for Experts In...</h1>
<div style="width:960px; margin: 0px auto;"/>
<input type="text" id="query1" style="width:70%;padding:8px;margin:0px 5px;" value="sql"/><input type="button" id="button1" value="Go" /><br/>
</p>
<div id="results1">
</div>
<br/>
<br/>
<br/>
<br/>
<br/>
</div>
<aside class="notes">
search:sql
facet.field=author
"Ilia Alshanetsky" - Postgres committer
"Wez Furlong"- oracle OCI driver for PHP
"Daniel Morgan" - oracle dba consultant
"Ulf Wendel" - principal on mysql
"Felipe Pena" - MS Sql driver for php?
          </aside>
        </section>
        <section>
        <h2>Corporate Open Source...</h1>
<div style="width:960px; margin: 0px auto;"/>
<input type="text" id="query2" style="width:70%;padding:8px;margin:0px 5px;" value="microsoft"/><input type="button" id="button2" value="Go" /><br/>
</p>
<div id="results2">
</div>
<br/>
<br/>
<br/>
<br/>
<br/>
</div>
<aside class="notes">
* At a Conference - MS says they support Open Source now because of Azure
* Azure - commercially available 2010
          </aside>
        </section>
        <section>
          <h2>Motivation</h2>
          <p>
            Who is an expert in technology X?
          </p>
          <aside class="notes">
            <li>In some large companies, it’s also helpful to know how many engineers have worked with a particular technology, and what the trends are. For instance, Microsoft’s MSDN program awards points based on certifications of developers in the organization, so it’s useful to know if you have a lot of certified C# developers, or people who could go get a certification if needed.</li>
            <li>You can answer these types of questions by counting the number of commits a person has that reference a client or tool, which is essentially faceted search. If you imagine the UI for Amazon, on the left hand side it shows you categories and category counts for fields that apply to items in your search results, such as the genre or studio for a movie. What we are going to do here is to build a facet for an author or other commit attributes. </li>
            <li>This is different from a typical search, where you're looking for the commits individually, which serves different use cases than I'm after here. This technique works especially well if you company is diligent about what you put in commits; clearly counting numbers of commits will not be helpful if your average commit is hundreds of files and infrequent.</li>
          </aside>
        </section>
        <section>
          <h2>Solr - Schema</h2>
          <pre><code data-trim>
&lt;field name="id" type="string" indexed="true" stored="false" required="true" /&gt; 
&lt;field name="author" type="git_author" indexed="true" stored="true" required="true" /&gt; 
&lt;field name="company" type="string" indexed="true" stored="true" required="true" /&gt; 
&lt;field name="year" type="string" indexed="true" stored="true" required="true" /&gt; 
&lt;field name="email" type="string" indexed="true" stored="false" required="true" /&gt; 
&lt;field name="message" type="string" indexed="true" stored="false" required="true" /&gt; 
&lt;field name="search" type="text_general" indexed="true" stored="false" required="false" /&gt; 
    </code></pre>
          <aside class="notes">
            <li>When you define the schema, you tell Solr what index information you want to store with columns - for instance, if you want to retrive data, or just use it in search.</li>
            <li>There are various other things you can turn off</li>
          </aside>
        </section>
          
        <section>
          <h2>Stats for Live Demo</h2>
          <ul>
                  <li>2.1 GB Git History => 132 MB Solr Repository</li>
                  <li>Index of 232,839 commits</li>
                  <li>Drupal, Git, Lucene, Lucene.NET, Solr, Mono, Node.JS, PHP, Postgres, Vagrant</li>
          </ul>
          <aside class="notes">
          </aside>
        </section>
        <section>
          <h2>Wingspan Source Code</h2>
          <ul>
            <li>2,000 MB Git Archive => 90 MB Solr Index</li>
            <li>Works well for finding project leads</li>
            <li>Works well for finding technology experts</li>
          </ul>
          <aside class="notes">
          <li>Built over a few hours/days during a conference</li>
          </aside>
        </section>
        <section>
          <h2>Github Archive</h2>
          <ul>
            <li>202 GB Git Archive => 1 GB Solr Index</li>
            <li>18,000 repositories</li>
            <li>4,554,502</li>
            <li>2-3 hour conversion process</li>
            <li>Developer workstation (i7 / 16 GB RAM)</li>
          </ul>
          <aside class="notes">
            <li>Shows what you can do with a decent developer workstation</li>
            <li>3-4 hours tuning the process to load this</li>
          </aside>
          </section>
        <section>
        <h1>What is Full Text Search?</h1>
        <li>The dogs ran home => dog run home</li>
        <li>The dog runs home => dog run home</li>
          <aside class="notes">
            <li>Solr and Lucene are part of a class of products which are used for full-text search. The idea of full-text search is to use natural language processing techniques to massage text data into a form that lends itself to search. For instance, finding words which are similar and treating them as equivalents. </li>
          </aside>
        </section>
        <section>
          <h2>Full Text Search vs. Google</h2>
          <aside class="notes">
            <li>Google uses citations to influence rankings</li>
            <li>Lucene calculates a similary score between each query and document</li>
            <li>Math makes it so common words don't have much effect</li>
          </aside>
        </section>
        <section>
        <h2>What is Solr?</h2>
          <img src="solr.png" class="solrimage" />
          <aside class="notes">
          <li>Solr was built as an administration webapp around Lucene</li>
          <li>Lucene is just a bunch of Jars for doing full-text search.</li>
          <li>Solr adds replication, faceted search, caching, and highlighting.</li>
          </aside>
        </section>
        <section>
          <h1>How this is built</h1>
          <li>Front-end</li>
          <li>Back-end</li>
          <aside class="notes">      
          </aside>
        </section>
        <section>
          <h2>JSON API</h2>
          <p>
            /select/?q=search:(search:jug)
          </p>
          <p>
            &amp;version=2.2
          </p>
          <p>
            &amp;start=0&amp;rows=0
          </p>
          <p>
            &amp;indent=on
          </p>
          <p>
            &amp;facet=on
          </p>
          <p>
            &amp;facet.field=author
          </p>
          <p>
            &amp;facet.method=fc
          </p>
          <p>
            &amp;facet.limit=30
          </p>
          <p>
            &amp;wt=json
          </p>
          <aside class="notes">
            <li>Shown here are an example URL to Solr's REST API with faceting turne on, and an example of what the join syntax looks like. Solr comes with several query parsers. This implements some interesting functionality - rather than having a single query language which is detailed like SQL, there are applications which nest multiple query parsers.</li>
            <li>The join case allows you to join the table to itself - if you wanted to use this to join in an ACL, for instance, you could add new columns for the ACL options, and an identifier which tells you which type of object a row is, essentially using a partitioning concept to build sub-tables.</li>
            <li>facet.method has three options, which control the execution path. One starts with an enumarable field, and counts the intersection of fields. The default loops through documents which match the query summing as it goes. This can also be split across sub-indexes and then re-added.</li>
            <li>Return types - csv, xml, json, php, python, ruby, javabin</li>
          </aside>
        </section>
        <section>
          <h2>Building a facet UI</h2>
          <pre><code data-trim>
var url = 'http://solr/core1/select/?q=search:(search:jug)';
$.ajax({
  type: "GET",
  url: url,
  success: function(response){
    // display search results
});
</code></pre>
          <aside class="notes">
            <li>You can set up a simple UI with a tool like Twitter bootstrap, and retrieve search results using the Solr APIs. The APIs have to be configured so that certain endpoints are available. Here’s some example code, this is just typical jQuery code.</li>
          </aside>
        </section>
        <section>
          <h1>Backend ETL</h1>
          <ul>
            <li><i>E</i>xtracting Git Commits</li>
            <li><i>T</i>ransforming Commits</li>
            <li><i>L</i>oading data to Solr</li>
          </ul>
          <aside class="notes">
            <li>JGit</li>
            <li>SolrJ</li>
            <li>CVS2GIT</li>
            <li>Many others are possible</li>
          </aside>
        </section>
        <section>
          <h2>Why do you need Java code?</h2>
          <ul>
           <li>Extract data (e.g. PDF contents)</li>
           <li>Data transformations (email -> company)</li>
          </ul>
          <aside class="notes">
	    <li>Denormalized data</li>
	    <li>Row = document</li>
            <li>Document comes from PDF / Office / etc</li>
            <li>Must eval document yourself</li>
            <li>Fiddle with data as needed</li>
            <li>Solr Repositories resemble a single, denormalized database table, which they call a core. Each row in the table is referred to as a “document”, which comes from the idea that you may have indexed the contents of a PDF, HTML file, or Word document, although there is no particular requirement on what a document must be.</li>
            <li>Even though Solr refers to rows a documents, Solr index just stores text, so if you started with a bunch of PDFs you’d have to build your own processing pipeline to extract and process text from them.</li>
            <li>Some customizations can be done with configurations in Solr - if these work, they tend to be pretty easy to figure out, and when you can't figure them out quickly, you usually need to resort to code.</li>
          </aside>
        </section>
        <section>
          <h2>Extracting Data from Git</h2>
          <pre><code data-trim>
FileRepositoryBuilder builder = new FileRepositoryBuilder();
Repository repository = 
  builder.setGitDir(new File(path))
    .build();

RevWalk walk = new RevWalk(repository);

for (Ref ref : repository.getAllRefs().values()) {
  if ("HEAD".equals(ref.getName())) {
    walk.markStart(walk.parseCommit(ref.getObjectId()));
    break;
  }
}

for (RevCommit commit : walk) {
  ...
}
    </code></pre>
          <aside class="notes">
            <li>I settled on JGit to read git repositories, after testing a couple Java-based libraries. JGit is a re-implementation of git in Java, and differs from other libraries in that it doesnt wrap the command line interface. Wrapping the command line is fraught with peril, and was very flaky when I tested it, and OS specific.</li>         
          </aside>
        </section>
        <section>
          <h2>Extracting Data - Patches</h2>
          <pre><code data-trim>
// To fetch file diffs, we must provide a stream, where they will be written:
ByteArrayOutputStream out = new ByteArrayOutputStream();
DiffFormatter df = new DiffFormatter(out);
df.setRepository(repository);

List&lt;DiffEntry&gt; diffs = df.scan(parent.getTree(), commit.getTree());

// Each of these objects is one file in the commit
for (Object obj : diffs) {
  DiffEntry diff = (DiffEntry) obj;
  FileHeader fh = df.toFileHeader(diff);
  df.format(diff);
  String diffText = out.toString("UTF-8");

  // Reset the stream, so we can get each patch separately
  out.reset();

  ...
}
</code></pre>
          <aside class="notes">
            <li>JGit lets you generate patch sets, or patch sets with several lines of context. The API is a bit weird - pass it an output stream, and it will write a diff to the stream. The neat thing about this is it lets you control a bunch of options - checking for renames, number of lines of context.</li>
            <li>Example transformations include entity name normalization or extracting data from external systems like LDAP - for instance, you might validate that things you think are ticket numbers are valid. </li>
          </aside>
        </section>
        <section>
          <h2>Extracting Commit Metadata </h2>
          <pre><code data-trim>
// For this application, only find new or modified files
if (diff.getChangeType() == DiffEntry.ChangeType.MODIFY ||
    diff.getChangeType() == DiffEntry.ChangeType.ADD)) {

  // We have enough information now to get commit messages, 
  // file names, and author information
  System.out.println(diff.getNewPath());
  System.out.println(commit.getFullMessage());
  System.out.println(commit.getAuthorIdent().getName());
  System.out.println(commit.getAuthorIdent().getEmailAddress());
}
            </code></pre>
          <aside class="notes">
            <li>For this application, I look only for new or modified files.</li>
            <li>This API gives us enough information to get filenames, author information, and the commit message.</li>
            <li>DiffEntry is each change within the commit.</li>
          </aside>
        </section>
        <section>
          <h2>Transformations - Search Data</h2>
          <pre><code data-trim>
Pattern capitals = Pattern.compile(".*([a-z])([A-Z]).*");
Matcher m = capitals.matcher(file);

// myAbstractFactory => my Abstract Factory
String fileNameTokens = 
  m.replaceAll("\1 \2");

// /project/src/large_grid.js => project src large grid js
fileNameTokens = fileNameTokens
   .replace("/", " ")
   .replace("-", " ")
   .replace(".", " ")
   .replace("_", " ");
  
String search =
   commit.getFullMessage() + 
   " " + file;
    </code></pre>
          <aside class="notes">
            <li>This is putting metadata into the search (project name, file name, java class)</li>
            <li>The spaces are significant because Solr is going to use these as token delimeters.</li>
            <li>In what I built, I've included filenames and paths in the data that you can search in solr, because sometimes these indicate client names or features that someone might have worked on.</li>
            <li>Remove large commits</li>
          </aside>
        </section>
        <section>
          <h2>Transformations - Companies</h2>
          <pre><code data-trim>
// x.y@google.com => google.com
String company = emailAddress.split("@")[1];

if (company.contains("."))
{	
  // abc.com => google
  company = company.substring(0, company.lastIndexOf("."));
}
			
return company;
    </code></pre>
          <aside class="notes">
          <li>Here we see the code we use to extract the company name from email addresses.</li>
          </aside>
        </section>
        <section>
        <h2>Transformations - Commit Footers</h2>
        <ul>
          <li>Signed-off-by</li>
          <li>Acked-by</li>
          <li>Reported-by</li>
          <li>Tested-by</li>
          <li>CC, Cc</li>
          <li>Bug</li>
        </ul>
        <aside class="notes">
        RevCommit.getFooterLines
        public final List
        <FooterLine>
        <li> There is a function called getFooterLines() which will parse entries from the end of the commit.</li>
        <li>This method splits all of the footer lines out of the last paragraph of the commit message, providing each line as a key-value pair, ordered by the order of the line's appearance in the commit message itself.</li>
        <li>Keys are alphanumeric, and can have dashes, the values can have anything but a new line</li>
        <li>Since there is built-in support in the tools, I thought it worth mentioning this, since this information can be useful if it's added consistently.</li>
        </aside>
        </section>
       <section>
          <h1>Loading Data in Solr</h1>
          <pre><code data-trim>
// Connections happen over HTTP:
HttpSolrServer server = new HttpSolrServer(
     "http://localhost:8080/solr");

Collection&lt;SolrInputDocument&gt; docs = new ArrayList&lt;SolrInputDocument&gt;();

SolrInputDocument doc = new SolrInputDocument();

// ID for the document contains enough information to find it in the source data:
doc.addField("id", remoteUrl + "." + commit.getId());

doc.addField("author", commit.getAuthorIdent().getName());
doc.addField("email", commit.getAuthorIdent().getEmailAddress());
doc.addField("message", commit.getFullMessage());

// Any data we let the user search against is included in this value:
doc.addField("search", search);
</code></pre>
          <aside class="notes">
            <li>Need to remove nulls</li>
            <li>Specify boost</li>
            <li>Note multiple columns - can facet on one, and search on the other</li>
          </aside>
          </section>
         <section>
          <h2>Loading Data - Threading</h2>
          <pre><code data-trim>
File[] files = new File("repositories\\").listFiles();
String lastRepository = getLastRepository(); // resume after failure

int i = 0;
int numThreads = Runtime.getRuntime().availableProcessors();
        
for (File f : files) {
  if (i++ % numThreads == _myIndex) { // load every nth repository
    String filename = f.getAbsolutePath() + "\\.git";
            
    convertRepo(server, filename);
  }
  i++;
}
    </code></pre>
          <aside class="notes">
            <li>Currently I have the application set up to run with several threads</li>
            <li>Most of the time is spent loading git data</li>
            <li>For this process, order of operations doesn't matter. But for some people it does.</li>
          </aside>
        </section>
        <section>
          <h2>Setting up Solr</h2>
          <ul>
            <li>Download Solr</li>
            <li>Modify schema.xml</li>
            <li>Start Jetty: <br/>java -jar start.jar</li>
          </ul>
          <aside class="notes">
          </aside>
        </section>
          <section>
          <h2>Solr - Limitations</h2>
          <ul>
            <li>Don't expose Solr publically</li>
            <li>Don't rely on joins</li>
            <li>Roll your own ACLs, if needed</li>
          </ul>
          <aside class="notes">
            <li>Solr aims to be as fast at returning results as possible. It avoids features which slow this down; for instance, if you want to enforce ACLs on items in search results, you need to add columns and figure out how to populate them on your own. It has very limited support for inner joins, which allows for a way to implement more complex ACL structures if you do need them.</li>
            <li>Solr also assumes that you secure the machine it lives on - it’s not intended to directly face the internet, although for simple examples I’ve done that, but there is a risk that someone could modify your repository.</li>
          </aside>
        </section>
       <section>
          <h1>Inspiration</h1>
          <ul>
            <li>High Volume Indexing</li>
            <li>Parsing Code Comments</li>
            <li>Testing</li>
            <li>Concepts for similar tools</li>
          </ul>
        </section>
        <section>
          <h2>Indexing Many Repositories</h2>
<img src="windows-cursor.gif" class="loadingimage" />
<img src="files2.png"  class="loadingimage" />
          <aside class="notes">
            <li>If you do something like this, you will discover that a folder with, say, a half million files, is a troublesome structure. Windows can open this, but it's very slow to even list the files. On a different project, I got up to a half million files in a folder, and was forced to come up with a different structure.</li>
          </aside>
          </section>
          <section>
          <h2>Indexing Many Repositories</h2>
<img src="files.png"  class="loadingimage" />
          <aside class="notes">
          <li>You can do a SHA or MD5 hash and take 2-4 characters</li>
          <li>One or two level deep folder hierarchy of this</li>
          <li>Makes it easier to split up data for a test, if you have a lot</li>          
          </aside>
        </section>
        <section>
        <h2>Constraints on ETL processes</h2>
        <ul>
                <li>
                Real-time updates are hard
                </li>
                <li>
                Plan for power failures
                </li>
                <li>
                Plan for reloading data
                </li>
        </ul>
          <aside class="notes">
            <li>An ETL process that does a lot of work may take hours, weeks, or months to finish.</li>
            <li>A natural consequence of this is that you may need to consider inevitable power failures. This is true of webapps too, but typically that means you lose a user's session, which is different from interrupting something like a long SQL statement.</li>
            <li>Even if you don't have something that takes weeks, the time to generate final data is an issue, as it limits the developer's ability to cycle through fixes.</li>
          </aside>
        </section>
        <section>
          <h2>Testing ETL processes</h2>
          <img src="diff.png" class="diffimage" />
          <aside class="notes">
            <li>Use real data - finds edge cases</li>
            <li>Consequently, you want to be able to load sample data, which ideally is a subset of a production system. Data you make up on your own is ok, if you are just starting out, but really does not reflect the full potential for edge cases found in real data.</li>
            <li>Once you have a running system, it's very helpful if you are able to run side-by-side processes to test new and old versions side-by-side- I worked with someone once on a reporting system, he told me that at a bank he'd worked at, they would run new versions of the code side-by-side with the old for a month, to not discrepancies.</li>
            <li>Use a key that lets you get back to the original data, rather than a sequence number</li>
          </aside>
        </section>
        <section>
          <h2>Parsing Comments</h2>
          <pre><code data-trim>        
@@ -1,9 +1,9 @@
/* 
* Here is a multi-line comment.
*/
String a = "x"; // here is a comment at the end of a line

-String b /* a comment in a line */ = "y"; 
+String b /* a <b>long</b> comment in a line */ = "y"; 

// Commented out code:
// String c = "d";       
</code></pre>
          <aside class="notes">
	    <li>Haven't tried to parse code yet - couple example challenges</li>
	    <li>Comments show challenges of attributing which work a person did within a file to what they're really working on</li>
	    <li>Would have to build parse tree for code and compare before/after</li>
            <li>At this point, you may have noticed that I haven't attempted to parse code - however, I have given a lot of thought to why I don't want to parse code, or at least handle it as a special case of unstructured text. What I want to show now is a couple interesting theoretical challenges, which I think provide some compelling cases of why it can be really challenging to get an ETL process or a data migration project to be successful.</li>
            <li>Code comments also provide good insight into who worked on what. Obviously these aren't code, but often contain valuable information, and should be relatively easy to detect, at least in most languages which descend from C. In a source file, you could detect these by looking for lines which contain two slashes, or the slash-star form for multi-line comments.
            </li>
          </aside>
        </section>
        <section>
        <h2>JSX</h2>
          <pre><code data-trim>
var MarkdownEditor = React.createClass({
  render: function() {
    return (
      &lt;div className="MarkdownEditor"&gt;
        &lt;h3&gt;Input&lt;/h3&gt;
        &lt;div
          className="content"
          dangerouslySetInnerHTML={{
            __html: converter.makeHtml(this.state.value)
          }}
        /&gt;
      &lt;/div&gt;
    );
  }
});
</code></pre>
          <aside class="notes">
            <li>If you felt that example was contrived, here's an illustrative snippet of actual code, which I got from the Documentation for JSX, which is a language that compiles to Javascript</li>
            <li>With all the new languages like this, you'd be hard press to build a generalized soluation.</li>
            <li>Many large projects also have a DSL</li>
          </aside>
        </section>
        <section>
        <h1>Variations</h1>
        <ul>
                <li>Lots of free data, especially Government - PACER extracts, U.S. Code, Philly Code</li>
                <li>MS Exchange / Sharepoint</li>
                <li>D3.js, Tilemill</li>
        </ul>
          <aside class="notes">
          <ul>
                  <li>Lots of datasets available now</li>
                  <li>Lots of data hidden inside companies</li>
                  <li>Nice graphing library made by artists for interactive infographics</li>
            <ul>
          </aside>
          </section>
        <section>
        <h1>References</h1>
	<p>
        <a href="https://github.com/garysieling/git-solr-talk">https://github.com/garysieling/git-solr-talk</a>
	</p>
	<p>
        <a href="https://github.com/garysieling/solr-git">https://github.com/garysieling/solr-git</a>
	</p>
          <aside class="notes">
          </aside>
        </section>
          
        <section>
          <h1>THE END</h1>
          <h3>BY <a href="http://www.garysieling.com">Gary Sieling</a> / <a href="http://twitter.com/garysieling">@garysieling</a></h3>
          <aside class="notes">
            The unoptimized Solr index for the Wingspan source code is 90 MB of commits versus 2,000 MB of git history. The trade-offs that make Solr indexes small also make it fast, even without any kind of scaling infrastructure.
          </aside>
        </section>
      </div>
    </div>
    <script src="js/jquery.js"></script>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    <script>
      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
      
        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
      
        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
});
$("#query0").keyup(function(event){
    if(event.keyCode == 13){
        $("#button0").click();
        return false;
    }
});

$("#query1").keyup(function(event){
    if(event.keyCode == 13){
        $("#button1").click();
        return false;
    }
});

$("#query2").keyup(function(event){
    if(event.keyCode == 13){
        $("#button2").click();
        return false;
    }
});



$(document).ready(function() {
  $('#button0').click(function() {
    search('#query0', '#results0',  'search', 'author', 'count', 0);
    return false;
  });
        
  $('#button1').click(function() {
    search('#query1', '#results1',  'search', 'author', 'count', 0);
    return false;
  });

  $('#button2').click(function() {
    search('#query2', '#results2',  'company', 'year', 'index', 10);
    return false;
  });


  function search(searchBox, searchResults, searchOn, facetOn, facetOrder, facetOffset)
  { 		
    var q = $(searchBox)[0];
    $(searchResults).innerHTML='Loading...';

    var url = 
      'http://localhost:8983/solr/collection1/select?q='  +
       searchOn +
      '%3A'  + 
      q.value +
      '&rows=0&wt=json&indent=true&facet=true&facet.field=' + 
      facetOn +
      '&facet.limit=5' +
      '&facet.sort=' + facetOrder +
      '&facet.offset=' + facetOffset;
    $.ajax({
      type: "GET",
      url: url,
      success:
        function(response){
          var data = JSON.parse(response);
          var records = data.facet_counts.facet_fields[facetOn];

          var count = Math.max(records.length, 11);
          var html = '<center><table style="text-align:left;">';
          if (!records || records.length === 0)
          {
            html+= "<tr><td>No results found.</td></tr>";
          }

          var count = 0;
          for (var i = 1; i < records.length; i+=2)
          {
            count += records[i];
          }
          for (var i = 0; i < records.length; i+=2)
          {
            var auth = records[i];
          
            html += '<tr><td>' + auth + '</td><td>(' + records[i+1] + ")</td><td>";

            for (var j = 0; j < 100 * (records[i+1])/(count+1); j++)
            {
              html += '.';
            }
            html += '</td></tr>';	
          }
console.log(html);

html += '</table></center>';
        $(searchResults)[0].innerHTML=html;
      }
    });
  }
});
    </script>
  </body>
</html>

