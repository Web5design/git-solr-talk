<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Mining Source Code Repositories with Solr</title>
    <meta name="description" content="Importing Git Repositories into a Solr Index">
    <meta name="author" content="Gary Sieling">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">
    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">
    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
      document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>
    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
    <style>
      .reveal pre {
      width:110%;
      }

      #button1 {
        padding: 7px 25px;
        background: #4479BA;
        color: #FFF;
      }
    </style>
  </head>
  <body>
    <div class="reveal">
      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Mining Source Code Repositories with Solr</h1>
          <h3>Exploring Full-Text Search</h3>
          <p>
            <a href="http://www.garysieling.com/jug">www.garysieling.com/jug</a>
          </p>
          <p>
            <small><a href="http://www.garysieling.com">Gary Sieling</a> / <a href="http://twitter.com/garysieling">@garysieling</a></small>
          </p>
          <aside class="notes">
            <li>In this talk I’m going to discuss full-text indexing in Solr, which is an open-source Java tool that wraps Lucene. I’ll show you how to build up an example project which indexes the contents of git source code repositories. </li>
            <li>Some of you may be familiar with products like Atlassian’s Fisheye, or code search engines like Krugle, or used the Github search. The nice thing about these is they let you do more than the out-of-the box 'blame' tool in your version control software, like finding out who ported a feature between branches, or searching both git and cvs repositories. </li>
            <li>Since there are mature products for this type of thing, this talk isn’t intended to be a replacement for them; rather it’s a well contained way to understand full-text search, and to explore the design challenges in building ETL processes.</li>
          </aside>
        </section>
        <section>
          <h2>Overview</h2>
          <ul>
          <li>Tool Demo</li>
          <li>How-to</li>
          <li>Inspiration</li>
          </ul>
          <aside class="notes">
            <li>Since Wingspan does some consulting projects, it’s often valuable for engineers to look at a change and know if it was done for a particular client - for instance, if an API feature does not appear to be used, it may actually be in use by a client. That said, the person who may have done the change may not be the most knowledgeable about it, and you'd prefer to know who the lead for a project was.</li>
            <li>Similarly, when someone calls the company, it’s also helpful to know who to route phone calls to - especially since the person who picked up the phone could be a marketing or administrative person receiving the call and not someone with knowledge of the project.</li>
          </aside>
        </section>
        <section>
          <h2>Motivation</h2>
          <p>
            Which engineer worked on a particular client/project/technology? 
          </p>
          <aside class="notes">
            <li>Since Wingspan does some consulting projects, it’s often valuable for engineers to look at a change and know if it was done for a particular client - for instance, if an API feature does not appear to be used, it may actually be in use by a client. That said, the person who may have done the change may not be the most knowledgeable about it, and you'd prefer to know who the lead for a project was.</li>
            <li>Similarly, when someone calls the company, it’s also helpful to know who to route phone calls to - especially since the person who picked up the phone could be a marketing or administrative person receiving the call and not someone with knowledge of the project.</li>
          </aside>
        </section>
        <section>
        <h1>Demo (Authors)</h1>

<div style="padding-top:50px; width:960px; margin: 0px auto;"/>
<h3>
Search for experts in...
</h3>
<input type="text" id="query1" style="width:70%;padding:8px;margin:0px 5px;"/><input type="button" id="button1" value="Go" /><br/>
<p>Examples: <a href="#">nodejs</a>, <a href="#">javascript</a>, <a href="#">linux</a>, <a href="#">orm</a>, <a href="#">intellij</a>.
</p>
<br/>
<div id="results1">
</div>
</div>
        


          <aside class="notes">
          </aside>
        </section>
        <section>
          <h2>Motivation</h2>
          <p>
            Corporate open source contributions
          </p>
          <aside class="notes">
            <li>With the advent of Microsoft's cloud hosting, they've been claiming a new era of contributing to open-source, to make things like PHP work on Windows.</li>
          </aside>
        </section>
        <section>
          <h1>Demo (Companies)</h1>
          <aside class="notes">
          </aside>
        </section>
        <section>
          <h2>Motivation</h2>
          <p>
            Who is an expert in technology X?
          </p>
          <aside class="notes">
            <li>In some large companies, it’s also helpful to know how many engineers have worked with a particular technology, and what the trends are. For instance, Microsoft’s MSDN program awards points based on certifications of developers in the organization, so it’s useful to know if you have a lot of certified C# developers, or people who could go get a certification if needed.</li>
            <li>You can answer these types of questions by counting the number of commits a person has that reference a client or tool, which is essentially faceted search. If you imagine the UI for Amazon, on the left hand side it shows you categories and category counts for fields that apply to items in your search results, such as the genre or studio for a movie. What we are going to do here is to build a facet for an author or other commit attributes. </li>
            <li>This is different from a typical search, where you're looking for the commits individually, which serves different use cases than I'm after here. This technique works especially well if you company is diligent about what you put in commits; clearly counting numbers of commits will not be helpful if your average commit is hundreds of files and infrequent.</li>
          </aside>
        </section>
        <section>
          <h2>Versions of this tool</h2>
          <p>
          </p>
          <aside class="notes">
            <li>These two use cases are problems we've see at Wingspan, but they are still somewhat contrived for our company, as we're small enough that these problems are well-answered by expert knowledge rather than a tool. When we tested this tool as a proof of concept, we were able to get answers that were strikingly accurate to our organization.</li>
          </aside>
        </section>
        <section>
          <h2>Using Github</h2>
          <aside class="notes">
            <li>After I set this up with Wingspan source code, I also downloaded around eighteen thousand repositories from github. Once loaded, this was about four million commits, although this is quite small in comparison to Github itself.</li>
          </aside>
        </section>
        <section>
          <h1>Full Text Search vs. Google</h1>
          <aside class="notes">
            <li>Solr and Lucene are part of a class of products which are used for full-text search. The idea of full-text search is to use natural language processing techniques to massage text data into a form that lends itself to search. For instance, finding words which are similar and treating them as equivalents. </li>
            <li>Typically search results are ordered by a relevance function, which is determined by word frequencies. Frequently occurring words have a small effect, and rare words matching between a document and a query have a large effect - the actual calculation is done with vector multiplication.</li>
            <li>This is a different understanding of search from the techniques that Google pioneered, which use citations to effect ranking. If you're interested in combining that type of search with Lucene, there is an apache project called Nutch, although I haven't used it, so I can't speak to how well it works.</li>
          </aside>
        </section>
        <section>
          <h1>What is Solr?</h1>
          <aside class="notes">
            <li>Solr was built as an administration webapp around Lucene, which is just a bunch of Jars for doing full-text search. Solr also adds some nice customizations, like replication, faceted search, caching, and highlighting. From talking to people at a Solr conference, I got the impression as well that when the lucene team doesn’t work at the pace they want, they will add features to Solr, so sometimes there is some overlap in functionality.</li>
            <li>As a project, the Solr team has recently been billing itself as a NoSQL database, and while I think that is a bit of marketing, it does perform very well and fits in the category loosely, as it can be fairly flexible about schemas, avoids joins, and has some horizontal scaling features.</li>
          </aside>
        </section>
        <section>
          <h2>How this is built</h2>
          <li>Front-end</li>
          <li>Back-end</li>
          <aside class="notes">      
          </aside>
        </section>
        <section>
          <h2>Getting search results through JSON</h2>
          <p>
            /select/?q=search:(search:jug)
          </p>
          <p>
            &amp;version=2.2
          </p>
          <p>
            &amp;start=0&amp;rows=0
          </p>
          <p>
            &amp;indent=on&amp;facet=on
          </p>
          <p>
            &amp;facet.field=author
          </p>
          <p>
            &amp;facet.method=fc
          </p>
          <p>
            &amp;facet.limit=30
          </p>
          <p>
            &amp;wt=json
          </p>
          <aside class="notes">
            <li>Shown here are an example URL to Solr's REST API with faceting turne on, and an example of what the join syntax looks like. Solr comes with several query parsers. This implements some interesting functionality - rather than having a single query language which is detailed like SQL, there are applications which nest multiple query parsers.</li>
            <li>The join case allows you to join the table to itself - if you wanted to use this to join in an ACL, for instance, you could add new columns for the ACL options, and an identifier which tells you which type of object a row is, essentially using a partitioning concept to build sub-tables.</li>
            <li>facet.method has three options, which control the execution path. One starts with an enumarable field, and counts the intersection of fields. The default loops through documents which match the query summing as it goes. This can also be split across sub-indexes and then re-added.</li>
            <li>Return types - csv, xml, json, php, python, ruby, javabin</li>
          </aside>
        </section>
        <section>
          <h2>Building a facet UI</h2>
          <pre><code data-trim>
var url = 'http://solr/core1/select/?q=search:(search:jug)';
$.ajax({
	type: "GET",
	url: url,
  success: function(response){
    // display search results
});
</code></pre>
          <aside class="notes">
            <li>You can set up a simple UI with a tool like Twitter bootstrap, and retrieve search results using the Solr APIs. The APIs have to be configured so that certain endpoints are available. Here’s some example code, this is just typical jQuery code.</li>
          </aside>
        </section>
        <section>
          <h2>Backend ETL</h2>
          <ul>
            <li>Extracting Git Commits</li>
            <li>Transforming Commits</li>
            <li>Loading data to Solr</li>
          </ul>
          <aside class="notes">
            <li>We need to do three things</li>
            <li>I'm going to show how to build up an ETL process in Java to build this tool using two libraries, SolrJ and JGit. While this talk is focused on extracting data from git, it's fairly easy to combine data across various data sources, since we're going to write Java code to extract data and populate an index. There are also suitable tools to convert older repositories into git data, such as cvs2git, which is suitable for this purpose.</li>
          </aside>
        </section>
        <section>
          <h1>Why do you need Java code?</h1>
          <aside class="notes">
            <li>Solr Repositories resemble a single, denormalized database table, which they call a core. Each row in the table is referred to as a “document”, which comes from the idea that you may have indexed the contents of a PDF, HTML file, or Word document, although there is no particular requirement on what a document must be.</li>
            <li>If you include the contents of a PDF and other metadata, by default these have equal weight, unless you tell Solr otherwise. In fact, the Solr index just stores text, so if you started with a bunch of PDFs you’d have to build your own processing pipeline to extract and process text from them.</li>
          </aside>
        </section>
        <section>
          <h2>Initializing a git repository</h2>
          <pre><code data-trim>
FileRepositoryBuilder builder = new FileRepositoryBuilder();
Repository repository = 
  builder.setGitDir(new File(path))
    .readEnvironment() 
    .findGitDir() 
    .build();

RevWalk walk = new RevWalk(repository);
    </code></pre>
          <aside class="notes">
            <li>There are a few different ways to push this data into a Solr repository, depending on the nature of the application. For instance, Solr has a class call the Data Import Handler, which can connect directly to a database and pull information in - for some people this works well, although I was advised that if you can't get this working in an hour or two, you'll never get it working, and should move on to another approach.</li>
            <li>I settled on JGit to read git repositories, after testing a couple Java-based libraries. JGit is a re-implementation of git in Java, and differs from other libraries in that it doesnt wrap the command line interface. Wrapping the command line is fraught with peril, and was very flaky when I tested it, and OS specific.</li>
            <li>It is also the basis for the Eclipse plugin EGit, so you know that there is a team mantaining it. Reading repositories is by far the slowest part of this script. JGit only reads what you ask for— it is much faster to read just the commit history, without file diffs.</li>
            <li>readEnvironment - read environment variables</li>
            <li>findGitDir - scan up the file system tree</li>
          </aside>
        </section>
        <section>
          <h2>Branches</h2>
          <pre><code data-trim>
for (Ref ref : repository.getAllRefs().values()) {
 try {
   System.out.println(ref.getObjectId()); 
   System.out.println(ref.getName()); // e.g. "HEAD"
   System.out.println(ref.isPeeled()); // "annotated" tag
   System.out.println(ref.isSymbolic()); // pointer instead of a SHA
  } catch (InvalidObjectException notACommit) {
    System.out.println("err");
    continue;
  }
}

for (RevCommit commit : walk) {
  ...
}
    </code></pre>
          <aside class="notes">
            <li>How do you find HEAD / master?</li>
            <li>How do you walk down branches?</li>
            <li>Can you just get one commit here?</li>
            <li>Can you get rid of the loop?</li>
            <li>Interesting complexity here - tags can be annotated, i.e. securely signed with author info</li>
          </aside>
        </section>
        <section>
          <h2>Patches</h2>
          <pre><code data-trim>
// To fetch file diffs, we must provide a stream, where they will be written:
ByteArrayOutputStream out = new ByteArrayOutputStream();
DiffFormatter df = new DiffFormatter(out);
df.setRepository(repository);

List&lt;DiffEntry&gt; diffs = df.scan(parent.getTree(), commit.getTree());

// Each of these objects is one file in the commit
for (Object obj : diffs) {
  DiffEntry diff = (DiffEntry) obj;
  FileHeader fh = df.toFileHeader(diff);
  df.format(diff);
  String diffText = out.toString("UTF-8");

  // Reset the stream, so we can get each patch separately
  out.reset();
}
</code></pre>
          <aside class="notes">
            <li>JGit lets you generate patch sets, or patch sets with several lines of context. The API is a bit weird - pass it an output stream, and it will write a diff to the stream. The neat thing about this is it lets you control a bunch of options - checking for renames, number of lines of context.</li>
            <li>Once we get to this point, we can apply any transformations we'd want to do before sending data to solr. Some transformations are best done in code. Depending on your needs, you can alternattely write custom Solr classes that modify data on a column by column basis, or just fix the data before it gets into the index. The difference between these two approaches will vary depending on whether you want global state, like caching, whether data manipulation requires complex cross-column lookups, and whether the transformations must be applied to both queries and documents.</li>
            <li>Example transformations include entity name normalization or extracting data from external systems like LDAP - for instance, you might validate that things you think are ticket numbers are valid. Because I combined several git repositories into one full-text index, I needed to normalize author names (“gsieling” and “Gary Sieling”). I also found that many tools will add metadata to commits, for instance Intellij adds a line to commits identifying itself, so I added an artificial field identifying the commit author’s IDE of choice.</li>
          </aside>
        </section>
        <section>
          <h2>Commit Comments</h2>
          <pre><code data-trim>
// For this application, only find new or modified files
if (diff.getChangeType() == DiffEntry.ChangeType.MODIFY ||
    diff.getChangeType() == DiffEntry.ChangeType.ADD)) {

  // We have enough information now to get commit messages, 
  // file names, and author information
  System.out.println(diff.getNewPath());
  System.out.println(commit.getFullMessage());
  System.out.println(commit.getAuthorIdent().getName());
}
            </code></pre>
          <aside class="notes">
            <li>For this application, I look only for new or modified files.</li>
            <li>This API gives us enough information to get filenames, author information, and the commit message.</li>
            <li>DiffEntry is each change within the commit.</li>
          </aside>
        </section>
        <section>
          <h2>Transforming Git Data</h2>
          <pre><code data-trim>
Pattern capitals = Pattern.compile(".*([a-z])([A-Z]).*");
Matcher m = capitals.matcher(file);

// myAbstractFactory => my Abstract Factory
String fileNameTokens = 
  m.replaceAll("\1 \2");

// /project/src/large_grid.js => project src large grid js
fileNameTokens = fileNameTokens
   .replace("/", " ")
   .replace("-", " ")
   .replace(".", " ")
   .replace("_", " ");
  
String search =
   commit.getFullMessage() + 
   " " + file;
    </code></pre>
          <aside class="notes">
            <li>This is putting metadata into the search (project name, file name, java class)</li>
            <li>The spaces are significant because Solr is going to use these as token delimeters.</li>
            <li>In what I built, I've included filenames and paths in the data that you can search in solr, because sometimes these indicate client names or features that someone might have worked on.</li>
            <li>The disadvantage to this approach is that some words may occur then on many or all files.</li>
          </aside>
        </section>
        <section>
          <h2>Extracting Company Names</h2>
          <pre><code data-trim>
// x.y@google.com => google.com
String company = emailAddress.split("@")[1];

if (company.contains("."))
{	
  // abc.com => google
  company = company.substring(0, company.lastIndexOf("."));
}
			
return company;
    </code></pre>
          <aside class="notes">
            <li>
              In repositories from github, you can get a good approximation of companies that contribute to repositories by taking the “@x.com” from emails, although to do this successfully, you must remove common hosts like gmail, and exclude contributions by some bots. Some of the major ones can be hidden easily, but this takes some work to find a general solution.
          </aside>
        </section>
        <section>
        <h2>Commit Metadata</h2>
        <p>Signed-off-by</p>
        <p>Acked-by</p>
        <p>Reported-by</p>
        <p>Tested-by</p>
        <p>CC, Cc</p>
        <p>Bug</p>
        <aside class="notes">
        RevCommit.getFooterLines
        public final List
        <FooterLine>
        <li> There is a function called getFooterLines() which will parse entries from the end of the commit.</li>
        <li>This method splits all of the footer lines out of the last paragraph of the commit message, providing each line as a key-value pair, ordered by the order of the line's appearance in the commit message itself.</li>
        <li>Keys are alphanumeric, and can have dashes, the values can have anything but a new line</li>
        <li>Since there is built-in support in the tools, I thought it worth mentioning this, since this information can be useful if it's added consistently.</li>
        </aside>
        </section>
        <section>
          <h2>Setting up Solr</h2>
          <aside class="notes">
            <li>Drop in a .war</li>
            <li>Use Jetty/Tomcat</li>
            <li>Set configuration parameters to point to index</li>
            <li>Logging settings/jars</li>
            <li>Permissions for index writing</li>
            <li>Index is adjacent to config files</li>
            <li>Point to core(s)</li>
            <li>Drop in other jars if needed (e.g. to connect to jdbc drivers)</li>
          </aside>
        </section>
        <section>
          <h2>Solr - Limitations</h2>
          <aside class="notes">
            <li>Solr aims to be as fast at returning results as possible. It avoids features which slow this down; for instance, if you want to enforce ACLs on items in search results, you need to add columns and figure out how to populate them on your own. It has very limited support for inner joins, which allows for a way to implement more complex ACL structures if you do need them.</li>
            <li>Solr also assumes that you secure the machine it lives on - it’s not intended to directly face the internet, although for simple examples I’ve done that, but there is a risk that someone could modify your repository.</li>
          </aside>
        </section>
        <section>
          <h2>Solr - Schema</h2>
          <pre><code data-trim>
&lt;field name="id" type="string" indexed="true" stored="false" required="true" /&gt; 
&lt;field name="author" type="git_author" indexed="true" stored="true" required="true" /&gt; 
&lt;field name="email" type="string" indexed="true" stored="false" required="true" /&gt; 
&lt;field name="message" type="string" indexed="true" stored="false" required="true" /&gt; 
&lt;field name="search" type="text_general" indexed="true" stored="false" required="false" /&gt; 
    </code></pre>
          <aside class="notes">
            <li>A common use for copyField is to combine multiple fields into one, although you can also use it to truncate data.
            <li>multiValued - can be array valued for a document</li>
            <li>compressed - gzip / compressThreshold</li>
            <li>omitNorms / termVectors - can remove data needed for full text indexing</li>
            <li>omitTermFreqAndPositions / omitPositions - can remove data used for highlighting</li>
            <li>What is the purpose of the 'facet' type in solr?</li>
            <li>What is 'string' vs. text_general?</li>
          </aside>
        </section>
        <section>
          <h2>Sending Data to Solr</h2>
          <pre><code data-trim>
// Connections happen over HTTP:
HttpSolrServer server = new HttpSolrServer(
     "http://localhost:8080/solr");

Collection&lt;SolrInputDocument&gt; docs = new ArrayList&lt;SolrInputDocument&gt;();

SolrInputDocument doc = new SolrInputDocument();

// ID for the document contains enough information to find it in the source data:
doc.addField("id", remoteUrl + "." + commit.getId());

doc.addField("author", commit.getAuthorIdent().getName());
doc.addField("email", commit.getAuthorIdent().getEmailAddress());
doc.addField("message", commit.getFullMessage());

// Any data we let the user search against is included in this value:
doc.addField("search", search);
</code></pre>
          <aside class="notes">
            <li>Need to remove nulls</li>
            <li>Specify boost</li>
            <li>Note multiple columns - can facet on one, and search on the other</li>
          </aside>
        </section>
        <section>
          <h2>Threading Updates</h2>
          <pre><code data-trim>
File[] files = new File("repositories\\").listFiles();
String lastRepository = getLastRepository(); // resume after failure

int i = 0;
boolean start = false;
        
for (File f : files) {
  if (i++ % _numThreads == _myIndex) { // load every nth repository
    String filename = f.getAbsolutePath() + "\\.git";
            
    start = start || filename.contains(lastRepository);
    if (!start) 
      continue;
            
    convertRepo(server, filename);
  }
}
    </code></pre>
          <aside class="notes">
            <li>Inside run method</li>
            <li>For this application I've chosen to have multiple threads which loop over all the available repositories. If there are ten threads, each thread knows it's number, from 1 to 10 - the first thread picks up files 1, 11, 21, the second thread picks up files 2, 12, 22, and so on. This allows the threads to be killed and restart part way without any communication, although it won't help you if you download new data out of sequence.</li>
          </aside>
        </section>
        <section>
          <h2>Inspiration</h2>
          <ul>
            <li>High Volume Indexing</li>
            <li>Parsing Code Comments</li>
            <li>Testing</li>
          </ul>
        </section>
        <section>
          <h2>Indexing Large Numbers of Repositories</h2>
<img src="files.png" />
          <aside class="notes">
            <li>If you do something like this, you will discover that a folder with, say, 18,000 subfolders is a troublesome structure. Windows can open this, but it's very slow to even list the files. On a different project, I got up to a half million files in a folder, and was forced to come up with a different structure.</li>
            <li>An alternate approach is to do something like an MD5 hash of each filename, and use the first four characters to divide them up. I like to build a two level folder structure with this, because it resembles a B+ tree and shares it's properties, and for a half million files, each folder ends up with 5-10 items in it. If you use something like md5 you're basically randomly distributing files into folders, in a repeatable way, which gives you a nice way to sample arbitrary percentages of the dataset.</li>
            <li>It's worth noting as well that Github also has a nice API for inspecting repositories and pull back JSON, but we're going to do it a harder way here so we can see how it works.</li>
          </aside>
        </section>
        <section>
          <h2>Constraints on ETL processes</h2>
          <aside class="notes">
            <li>An ETL process that does a lot of work may take hours, weeks, or months to finish.</li>
            <li>A natural consequence of this is that you may need to consider inevitable power failures. This is true of webapps too, but typically that means you lose a user's session, which is different from interrupting something like a long SQL statement.</li>
            <li>Even if you don't have something that takes weeks, the time to generate final data is an issue, as it limits the developer's ability to cycle through fixes.</li>
          </aside>
        </section>
        <section>
          <h2>Testing ETL processes</h2>
          <aside class="notes">
            <li>Consequently, you want to be able to load sample data, which ideally is a subset of a production system. Data you make up on your own is ok, if you are just starting out, but really does not reflect the full potential for edge cases found in real data.</li>
            <li>Once you have a running system, it's very helpful if you are able to run side-by-side processes to test new and old versions side-by-side- I worked with someone once on a reporting system, he told me that at a bank he'd worked at, they would run new versions of the code side-by-side with the old for a month, to not discrepancies.</li>
            <li>Use a key that lets you get back to the original data, rather than a sequence number</li>
          </aside>
        </section>
        <section>
          <h2>Error Handing in ETL processes</h2>
          <aside class="notes">
            <li>For some types of tools, you probably don't want to fail, but note problems and move on, then incrementally reload parts that failed. This is especially true of data that is scraped or scanned.</li>
          </aside>
        </section>
        <section>
          <h2>Parsing Comments</h2>
          <pre><code data-trim>        
@@ -1,9 +1,9 @@
/* 
* Here is a multi-line comment.
*/
String a = "x"; // here is a comment at the end of a line

-String b /* a comment in a line */ = "y"; 
+String b /* a <b>long</b> comment in a line */ = "y"; 

// Commented out code:
// String c = "d";       
</code></pre>
          <aside class="notes">
            <li>At this point, you may have noticed that I haven't attempted to parse code - however, I have given a lot of thought to why I don't want to parse code, or at least handle it as a special case of unstructured text. What I want to show now is a couple interesting theoretical challenges, which I think provide some compelling cases of why it can be really challenging to get an ETL process or a data migration project to be successful.</li>
            <li>Code comments also provide good insight into who worked on what. Obviously these aren't code, but often contain valuable information, and should be relatively easy to detect, at least in most languages which descend from C. In a source file, you could detect these by looking for lines which contain two slashes, or the slash-star form for multi-line comments.
            <li>
            <li>In this slide I've shown a few of the testing scenarios, where comments appear in any part of a line. In fact, I haven't captured all the scenarios here, as there could be a multi-line comment that uses the two-slash style. The real point I want to make here though is that when you're looking at patches combined with authors, it's going to be tricky. You have to decide if you want to grab the whole comment, or just the part that changed, and whether you care about lines outside the commit, which you mights.</li>
          </aside>
        </section>
        <section>
          <pre><code data-trim>
var MarkdownEditor = React.createClass({
  render: function() {
    return (
      &lt;div className="MarkdownEditor"&gt;
        &lt;h3&gt;Input&lt;/h3&gt;
        &lt;div
          className="content"
          dangerouslySetInnerHTML={{
            __html: converter.makeHtml(this.state.value)
          }}
        /&gt;
      &lt;/div&gt;
    );
  }
});
</code></pre>
          <aside class="notes">
            <li>If you felt that example was contrived, here's an illustrative snippet of actual code, which I got from the Documentation for React, which is a new Javascript library for building UIs - you can see here that there is XML code inside the Javascript code, which gets compiled into something valid.</li>
            <li>I chose React because it's a new tool, and having nested languages in files is not a new concept, considering the number of Java string containing SQL, and I think it shows the challenge of keeping up with what's out there. Because of this challenge you probably want to avoid techniques that aren't going to be maintainable.</li>
            <li>Many large projects have some sort of custom DSL - any tool you'd build to parse code would have to deal with a lot of unrecognizable formats. I think that if you were going to tackle this, the best approach would be some loose regular expression style syntax highlighting tool, like the Vim plugins, since you'd have a chance at getting something useful.</li>
            <li>Say you still wanted to parse the code though - what could you do? You'd want to reconstruct the file as it was at the commit time, build a syntax tree, and diff the tree between commits, ideally maintaining line numbers for verification purposes.</li>
          </aside>
        </section>
        <aside class="notes">
          <li>Solr accepts queries through a REST API, which gives you an easy, ready-built API, if needed.  You can add, insert, update, delete any amount through the API.</li>
          <li>This shows which collection we want to hit</li>
          <li>The query, *:* brings back everything</li>
          <li>We can set paging parameters</li>
          <li>We can also set faceting parameters</li>
        </aside>
        </section>
        <section>
          <h2>Variations</h2>
          <aside class="notes">
            <li>You can also imagine importing data from other sources and combining it, such as a defect tracking systems, a Sharepoint site, or even old emails. In fact, depending how you spin this, you can end up with a variety of products- I once worked for a company that had a product that did this to find contact information, i.e. which lawyer in our firm has a contact at, say, eBay.</li>
            <li>Using Solr facets this way resembles a business intelligence systems used for reporting projects, in that it gives you counts across lots of denormalized data. Since Solr acts as a single table, I’m not sure if anyone uses it that way, but since it typically returns results quickly, I find myself tempted to use it rather than a relational database at times.</li>
          </aside>
        </section>
        <section>
          <h1>THE END</h1>
          <h3>BY <a href="http://www.garysieling.com">Gary Sieling</a> / <a href="http://twitter.com/garysieling">@garysieling</a></h3>
          <aside class="notes">
            The unoptimized Solr index for the Wingspan source code is 90 MB of commits versus 2,000 MB of git history. The trade-offs that make Solr indexes small also make it fast, even without any kind of scaling infrastructure.
          </aside>
        </section>
      </div>
    </div>
    <script src="js/jquery.js"></script>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    <script>
      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
      
        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
      
        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
});
$("#query1").keyup(function(event){
    if(event.keyCode == 13){
        $("#button1").click();
        return false;
    }
});

$(document).ready(function() {
  $('#button1').click(function() {
    search('#query1', '#results1');
    return false;
  });

  function search(searchBox, searchResults)
  { 		
    var q = $(searchBox)[0];
    $(searchResults).innerHTML='Loading...';
    var query = '';

    var url = 'sample.json';
    $.ajax({
      type: "GET",
      url: url,
      success:
        function(response){

          var data = JSON.parse(response);
          var records = data.facet_counts.facet_fields.author;

          var count = Math.max(records.length, 11);
          var html = '<table style="text-align:left;">';
          if (!records || records.length === 0)
          {
            html+= "<tr><td>No results found.</td></tr>";
          }

          var count = 0;
          for (var i = 1; i < records.length; i+=2)
          {
            count += records[i];
          }
          for (var i = 0; i < records.length; i+=2)
          {
            var auth = records[i];
          
            html += '<tr><td>' + auth + '</td><td>';

            for (var j = 0; j < 100 * (records[i+1])/(count+1); j++)
            {
              html += '.';
            }
            html += '</td></tr>';	
          }
console.log(html);

        html += '</table>';
        $(searchResults)[0].innerHTML=html;
      }
    });
  }
});
    </script>
  </body>
</html>

